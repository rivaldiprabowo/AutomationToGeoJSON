{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automation Excel to CSV and GeoJSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Application to Export Excel into CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Function Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_excel(file_path, output_folder):\n",
    "    # Load Excel workbook\n",
    "    wb = load_workbook(file_path, data_only=True)\n",
    "    \n",
    "    # Ensure output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    for sheet_name in wb.sheetnames:\n",
    "        ws = wb[sheet_name]\n",
    "\n",
    "        # Unmerge cells and fill values\n",
    "        for merge in list(ws.merged_cells):\n",
    "            ws.unmerge_cells(str(merge))\n",
    "            top_left = ws.cell(merge.min_row, merge.min_col).value\n",
    "            for row in range(merge.min_row, merge.max_row + 1):\n",
    "                for col in range(merge.min_col, merge.max_col + 1):\n",
    "                    ws.cell(row, col, top_left)\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        data = list(ws.values)\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        # Identify the header row (where \"NO\" appears)\n",
    "        header_index = df[df.apply(lambda x: x.astype(str).str.contains(\"NO\", case=False, na=False)).any(axis=1)].index[0]\n",
    "\n",
    "        # Use row 3 (index 2) as the header\n",
    "        df.columns = df.iloc[2].astype(str).str.strip()\n",
    "\n",
    "        # Remove empty columns\n",
    "        df = df.dropna(axis=1, how=\"all\")\n",
    "\n",
    "        # Drop the \"REKAP\" section if present\n",
    "        df = df.loc[:, ~df.columns.str.contains(\"REKAP\", case=False, na=False)]\n",
    "        df = df.drop(index=[0, 1, 4]).reset_index(drop=True)\n",
    "\n",
    "        # Merge the first two rows: Keep if same, else merge\n",
    "        merged_header = [a if a == b else f\"{a} {b}\" for a, b in zip(df.iloc[0], df.iloc[1])]\n",
    "\n",
    "        # Update DataFrame headers and remove first two rows\n",
    "        df.columns = merged_header\n",
    "        df = df.drop(index=[0, 1]).reset_index(drop=True)\n",
    "\n",
    "        # Construct full path for CSV file\n",
    "        csv_filename = os.path.join(output_folder, f\"{sheet_name}.csv\")\n",
    "        \n",
    "        # Save DataFrame as CSV\n",
    "        df.to_csv(csv_filename, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "        print(f\"✅ {sheet_name} exported to {csv_filename}\")\n",
    "\n",
    "    print(f\"✅ All sheets processed successfully! Files saved in: {output_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Run Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kanzi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openpyxl\\worksheet\\_reader.py:329: UserWarning: Conditional Formatting extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RAMBU exported to C:\\Users\\kanzi\\Documents\\Part Time Job\\Automation Codes\\check2\\RAMBU.csv\n",
      "✅ PJU exported to C:\\Users\\kanzi\\Documents\\Part Time Job\\Automation Codes\\check2\\PJU.csv\n",
      "✅ RPPJ exported to C:\\Users\\kanzi\\Documents\\Part Time Job\\Automation Codes\\check2\\RPPJ.csv\n",
      "✅ PAGAR PENGAMAN exported to C:\\Users\\kanzi\\Documents\\Part Time Job\\Automation Codes\\check2\\PAGAR PENGAMAN.csv\n",
      "✅ MARKA exported to C:\\Users\\kanzi\\Documents\\Part Time Job\\Automation Codes\\check2\\MARKA.csv\n",
      "✅ WARNING LIGHT exported to C:\\Users\\kanzi\\Documents\\Part Time Job\\Automation Codes\\check2\\WARNING LIGHT.csv\n",
      "✅ APILL exported to C:\\Users\\kanzi\\Documents\\Part Time Job\\Automation Codes\\check2\\APILL.csv\n",
      "✅ ZOSS exported to C:\\Users\\kanzi\\Documents\\Part Time Job\\Automation Codes\\check2\\ZOSS.csv\n",
      "✅ FAS PENYEBERANGAN exported to C:\\Users\\kanzi\\Documents\\Part Time Job\\Automation Codes\\check2\\FAS PENYEBERANGAN.csv\n",
      "✅ RAMBU PORTABLE exported to C:\\Users\\kanzi\\Documents\\Part Time Job\\Automation Codes\\check2\\RAMBU PORTABLE.csv\n",
      "✅ TRAFFIC CONE exported to C:\\Users\\kanzi\\Documents\\Part Time Job\\Automation Codes\\check2\\TRAFFIC CONE.csv\n",
      "✅ WATER BARRIER exported to C:\\Users\\kanzi\\Documents\\Part Time Job\\Automation Codes\\check2\\WATER BARRIER.csv\n",
      "✅ CERMIN TIKUNG exported to C:\\Users\\kanzi\\Documents\\Part Time Job\\Automation Codes\\check2\\CERMIN TIKUNG.csv\n",
      "✅ All sheets processed successfully! Files saved in: C:\\Users\\kanzi\\Documents\\Part Time Job\\Automation Codes\\check2\n"
     ]
    }
   ],
   "source": [
    "excel_file = r\"C:\\Users\\kanzi\\Documents\\Part Time Job\\Automation Codes\\02. CILEUNGSI - CIBINONG (CITEUREUP).xlsx\"  # Fill with the path file of excel\n",
    "export_folder = r\"C:\\Users\\kanzi\\Documents\\Part Time Job\\Automation Codes\\check2\"  # Fill with the path folder of export result\n",
    "flatten_excel(excel_file, export_folder) # Run the function!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Application to Export Excel into GeoJson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Function Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_column_names_unique(columns): #Function to ensure column names are unique by appending '_1', '_2'\n",
    "    seen = {}\n",
    "    new_columns = []\n",
    "    \n",
    "    for col in columns:\n",
    "        if col in seen:\n",
    "            seen[col] += 1\n",
    "            new_columns.append(f\"{col}_{seen[col]}\")  # Append a counter to duplicate columns\n",
    "        else:\n",
    "            seen[col] = 0\n",
    "            new_columns.append(col)\n",
    "    \n",
    "    return new_columns\n",
    "\n",
    "def clean_column_names(columns): #Standardize column names by capitalizing each word properly.\n",
    "    cleaned_columns = []\n",
    "    seen = {}\n",
    "\n",
    "    for col in columns:\n",
    "        col = str(col).strip()  # Remove extra spaces\n",
    "        col = \" \".join(word.capitalize() for word in col.split())  # Capitalize each word\n",
    "        \n",
    "        # Ensure uniqueness by appending a counter if needed\n",
    "        if col in seen:\n",
    "            seen[col] += 1\n",
    "            col = f\"{col} {seen[col]}\"\n",
    "        else:\n",
    "            seen[col] = 0\n",
    "        cleaned_columns.append(col)\n",
    "\n",
    "    return cleaned_columns\n",
    "\n",
    "def flatten_excel_to_geojson(file_path, output_folder): \n",
    "#Convert all sheets from an Excel file to GeoJSON, ensuring unique column names, drop unuse column, fix bad format of coordinate\n",
    "    \n",
    "    # Load workbook\n",
    "    wb = load_workbook(file_path, data_only=True)\n",
    "\n",
    "    for sheet_name in wb.sheetnames:\n",
    "        ws = wb[sheet_name]\n",
    "\n",
    "        # Unmerge cells and fill values\n",
    "        for merge in list(ws.merged_cells):\n",
    "            ws.unmerge_cells(str(merge))\n",
    "            top_left = ws.cell(merge.min_row, merge.min_col).value\n",
    "            for row in range(merge.min_row, merge.max_row + 1):\n",
    "                for col in range(merge.min_col, merge.max_col + 1):\n",
    "                    ws.cell(row, col, top_left)\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        data = list(ws.values)\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        # Identify the header row\n",
    "        header_index = df[df.apply(lambda x: x.astype(str).str.contains(\"NO\", case=False, na=False)).any(axis=1)].index[0]\n",
    "\n",
    "        # Use row 3 (index 2) as the header\n",
    "        df.columns = df.iloc[2].astype(str).str.strip()\n",
    "\n",
    "        # Remove empty columns\n",
    "        df = df.dropna(axis=1, how=\"all\")\n",
    "\n",
    "        # Drop \"REKAP\" section if present\n",
    "        df = df.loc[:, ~df.columns.str.contains(\"REKAP\", case=False, na=False)]\n",
    "        df = df.drop(index=[0, 1, 4]).reset_index(drop=True)\n",
    "\n",
    "        # Merge first two rows if needed\n",
    "        merged_header = [a if a == b else f\"{a} {b}\" for a, b in zip(df.iloc[0], df.iloc[1])]\n",
    "\n",
    "        # Ensure column names are unique\n",
    "        df.columns = make_column_names_unique(merged_header)\n",
    "\n",
    "        # Remove the first two rows used for headers\n",
    "        df = df.drop(index=[0, 1]).reset_index(drop=True)\n",
    "\n",
    "        # Normalize column names for consistent detection\n",
    "        df.columns = df.columns.str.lower().str.strip()\n",
    "\n",
    "        # Find Latitude & Longitude columns dynamically\n",
    "        lat_col = next((col for col in df.columns if \"latitude\" in col), None)\n",
    "        lon_col = next((col for col in df.columns if \"longitude\" in col), None)\n",
    "\n",
    "        if not lat_col or not lon_col:\n",
    "            print(f\"⚠️ Skipping '{sheet_name}' (No Latitude/Longitude columns)\")\n",
    "            continue  # Skip this sheet if Lat/Lon are missing\n",
    "\n",
    "        # Convert Lat/Lon to numeric first (forcing errors to NaN)\n",
    "        df[lat_col] = pd.to_numeric(df[lat_col], errors='coerce')\n",
    "        df[lon_col] = pd.to_numeric(df[lon_col], errors='coerce')\n",
    "\n",
    "        # Fix latitude and longitude values\n",
    "        def fix_coordinates(row):\n",
    "            \"\"\"Corrects lat/lon values if they are missing decimal points.\"\"\"\n",
    "            lat, lon = row[lat_col], row[lon_col]\n",
    "            \n",
    "            # Ensure values are numeric before applying conditions\n",
    "            if pd.notna(lat) and abs(lat) > 90:  # Latitude should be between -90 and 90\n",
    "                lat /= 1_000_000\n",
    "            if pd.notna(lon) and abs(lon) > 180:  # Longitude should be between -180 and 180\n",
    "                lon /= 1_000_000\n",
    "            \n",
    "            return pd.Series([lat, lon])\n",
    "\n",
    "        # Apply the fix function\n",
    "        df[[lat_col, lon_col]] = df.apply(fix_coordinates, axis=1)\n",
    "\n",
    "        # Remove rows where Lat/Lon are still missing\n",
    "        df = df.dropna(subset=[lat_col, lon_col]).reset_index(drop=True)\n",
    "\n",
    "        # Create GeoDataFrame\n",
    "        properties_cols = [col for col in df.columns if col not in [lat_col, lon_col]]\n",
    "        geometry = [Point(xy) if pd.notna(xy[0]) and pd.notna(xy[1]) else None for xy in zip(df[lon_col], df[lat_col])]\n",
    "\n",
    "        gdf = gpd.GeoDataFrame(df[properties_cols], geometry=geometry, crs=\"EPSG:4326\")\n",
    "\n",
    "        # Apply column renaming after creating the GeoDataFrame\n",
    "        gdf.columns = clean_column_names(gdf.columns)\n",
    "\n",
    "        # Drop invalid geometries\n",
    "        gdf = gdf.dropna(subset=['Geometry'])\n",
    "\n",
    "        # Ensure all column names are strings\n",
    "        gdf.columns = gdf.columns.astype(str)\n",
    "\n",
    "        # Remove unwanted \"None_\" and \"None\" columns\n",
    "        gdf = gdf.loc[:, ~gdf.columns.str.match(r\"^None$|None_\", na=False)]\n",
    "\n",
    "        # Remove \" None\" from remaining column names\n",
    "        gdf.columns = gdf.columns.str.replace(r\"\\sNone\\b\", \"\", regex=True).str.strip()\n",
    "\n",
    "        # Create output folder if it doesn't exist\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "        # Define output file path\n",
    "        output_path = os.path.join(output_folder, f\"{sheet_name}.geojson\")\n",
    "\n",
    "        # Export to GeoJSON\n",
    "        gdf.to_file(output_path, driver=\"GeoJSON\")\n",
    "\n",
    "        print(f\"✅ Saved: {output_path}\")\n",
    "\n",
    "    print(\"🎉 All sheets processed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Run Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kanzi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openpyxl\\worksheet\\_reader.py:329: UserWarning: Conditional Formatting extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: C:\\Users\\kanzi\\Documents\\Part Time Job\\Automation Codes\\check2\\RAMBU.geojson\n",
      "✅ Saved: C:\\Users\\kanzi\\Documents\\Part Time Job\\Automation Codes\\check2\\PJU.geojson\n",
      "✅ Saved: C:\\Users\\kanzi\\Documents\\Part Time Job\\Automation Codes\\check2\\RPPJ.geojson\n",
      "✅ Saved: C:\\Users\\kanzi\\Documents\\Part Time Job\\Automation Codes\\check2\\PAGAR PENGAMAN.geojson\n",
      "✅ Saved: C:\\Users\\kanzi\\Documents\\Part Time Job\\Automation Codes\\check2\\MARKA.geojson\n",
      "✅ Saved: C:\\Users\\kanzi\\Documents\\Part Time Job\\Automation Codes\\check2\\WARNING LIGHT.geojson\n",
      "✅ Saved: C:\\Users\\kanzi\\Documents\\Part Time Job\\Automation Codes\\check2\\APILL.geojson\n",
      "✅ Saved: C:\\Users\\kanzi\\Documents\\Part Time Job\\Automation Codes\\check2\\ZOSS.geojson\n",
      "✅ Saved: C:\\Users\\kanzi\\Documents\\Part Time Job\\Automation Codes\\check2\\FAS PENYEBERANGAN.geojson\n",
      "⚠️ Skipping 'RAMBU PORTABLE' (No Latitude/Longitude columns)\n",
      "⚠️ Skipping 'TRAFFIC CONE' (No Latitude/Longitude columns)\n",
      "⚠️ Skipping 'WATER BARRIER' (No Latitude/Longitude columns)\n",
      "✅ Saved: C:\\Users\\kanzi\\Documents\\Part Time Job\\Automation Codes\\check2\\CERMIN TIKUNG.geojson\n",
      "🎉 All sheets processed successfully!\n"
     ]
    }
   ],
   "source": [
    "excel_file = r\"C:\\Users\\kanzi\\Documents\\Part Time Job\\Automation Codes\\02. CILEUNGSI - CIBINONG (CITEUREUP).xlsx\"  # Fill with the path file of excel\n",
    "export_folder = r\"C:\\Users\\kanzi\\Documents\\Part Time Job\\Automation Codes\\check2\"  # Fill with the path folder of export result\n",
    "flatten_excel_to_geojson(excel_file, export_folder) # Run the function!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
